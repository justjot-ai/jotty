# RL Agent Selection + Planner/Architect - How They Work Together

**Date**: 2026-01-17
**Question**: "we also have planner which looks at available agents and then decides. how does that also fit together"

---

## ğŸ¯ TL;DR - Two SEPARATE Concerns

| Component | Purpose | Answers | When |
|-----------|---------|---------|------|
| **Q-Learning (RL)** | **Which agent to run next** | "Should we run Fetcher or Processor next?" | BEFORE selecting task |
| **Planner/Architect** | **Should this agent execute now** | "Should Fetcher proceed given current state?" | AFTER selecting task, BEFORE execution |

**They are COMPLEMENTARY, not competing!**

---

## ğŸ“Š Execution Flow

### **Without RL (Original Behavior)**

```
1. Get next task â†’ Uses fixed sequential order
2. Planner validates â†’ "Should this agent proceed?"
   â”œâ”€ Yes â†’ Execute agent
   â””â”€ No â†’ Block execution, mark as failed
3. Agent executes
4. Reviewer validates â†’ "Was output valid?"
```

### **With RL (New Behavior)**

```
1. Get next task â†’ Uses Q-value-based Îµ-greedy selection
   â”œâ”€ Get Q-values for all available agents
   â”œâ”€ Select best Q-value (70% of time)
   â””â”€ Select random (30% of time - exploration)

2. Planner validates â†’ "Should THIS agent proceed NOW?"
   â”œâ”€ Checks preconditions (does it have needed inputs?)
   â”œâ”€ Checks context (is this a good time?)
   â””â”€ Decision: proceed=True/False

3. IF Planner says proceed:
   â””â”€ Execute agent

4. Reviewer validates â†’ "Was output valid?"
   â”œâ”€ Check output quality
   â””â”€ Decision: valid=True/False

5. RL learns from outcome:
   â”œâ”€ If succeeded â†’ increase Q-value for (state, agent) pair
   â””â”€ If failed â†’ decrease Q-value
```

---

## ğŸ” Detailed Example: Data Pipeline

### **Scenario**: Process sales data (Fetch â†’ Process â†’ Visualize)

### **Episode 1** (Wrong Order - Visualizer First)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Q-LEARNING SELECTION (Iteration 1)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Available tasks: [Visualizer, Fetcher, Processor]          â”‚
â”‚ Q-values: Visualizer=0.50, Fetcher=0.50, Processor=0.50    â”‚
â”‚ Selection: Visualizer (random among equals)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. PLANNER VALIDATION                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Agent: Visualizer                                           â”‚
â”‚ Context: No data available (no Fetcher output yet)         â”‚
â”‚ Planner checks:                                             â”‚
â”‚   - "Does Visualizer have the data it needs?" â†’ NO          â”‚
â”‚   - "Is this the right time to visualize?" â†’ NO             â”‚
â”‚ Decision: should_proceed = FALSE (BLOCKED!)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. RL LEARNS FROM FAILURE                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ State: "No data fetched yet"                                â”‚
â”‚ Action: "Run Visualizer"                                    â”‚
â”‚ Reward: NEGATIVE (blocked by Planner)                      â”‚
â”‚ Q-value update: Visualizer Q-value â†“ (0.50 â†’ 0.48)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Q-LEARNING SELECTION (Iteration 2)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Available tasks: [Fetcher, Processor]                      â”‚
â”‚ Q-values: Fetcher=0.50, Processor=0.50                     â”‚
â”‚ Selection: Fetcher (random among equals)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. PLANNER VALIDATION                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Agent: Fetcher                                              â”‚
â”‚ Context: Start of pipeline (no dependencies)               â”‚
â”‚ Planner checks:                                             â”‚
â”‚   - "Does Fetcher have what it needs?" â†’ YES (query)        â”‚
â”‚   - "Is this the right time to fetch?" â†’ YES                â”‚
â”‚ Decision: should_proceed = TRUE (PROCEED!)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. AGENT EXECUTES                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Fetcher runs â†’ Fetches sales data â†’ Returns JSON           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. REVIEWER VALIDATION                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Check output: Valid JSON with sales data                   â”‚
â”‚ Decision: is_valid = TRUE                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. RL LEARNS FROM SUCCESS                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ State: "No data fetched yet"                                â”‚
â”‚ Action: "Run Fetcher"                                       â”‚
â”‚ Reward: POSITIVE (succeeded, produced valid data)          â”‚
â”‚ Q-value update: Fetcher Q-value â†‘ (0.50 â†’ 0.62)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ Learning Over Episodes

### **After 10 Episodes**:

```
Q-values learned:

State: "No data yet"
â”œâ”€ Run Visualizer â†’ Q = 0.35 (often blocked by Planner)
â”œâ”€ Run Fetcher â†’ Q = 0.75 (always succeeds, provides data)
â””â”€ Run Processor â†’ Q = 0.42 (blocked - needs Fetcher data first)

State: "Fetcher completed"
â”œâ”€ Run Visualizer â†’ Q = 0.38 (blocked - needs processed data)
â”œâ”€ Run Fetcher â†’ Q = 0.45 (redundant, already have data)
â””â”€ Run Processor â†’ Q = 0.80 (succeeds, uses Fetcher data)

State: "Processor completed"
â”œâ”€ Run Visualizer â†’ Q = 0.85 (succeeds, has processed data)
â”œâ”€ Run Fetcher â†’ Q = 0.40 (redundant)
â””â”€ Run Processor â†’ Q = 0.40 (redundant)
```

### **Result**: RL Learns Optimal Order

```
Episode 1-5:   Mixed (exploring)
Episode 6-15:  Fetcher first (60% of time)
Episode 16-30: Fetcher â†’ Processor â†’ Visualizer (80% of time)
Episode 31+:   Correct order 90%+ of time
```

---

## ğŸ¤ How They Complement Each Other

### **Q-Learning Provides**:
- âœ… **Strategic ordering** - Learns which agent sequences work best
- âœ… **Exploration** - Tries different orders to discover optimal patterns
- âœ… **Adaptation** - Adjusts to changing conditions over time

### **Planner Provides**:
- âœ… **Tactical validation** - Checks if NOW is the right time to run this agent
- âœ… **Safety** - Prevents agents from running without needed inputs
- âœ… **Context awareness** - Understands current state and dependencies

### **Together They Create**:
- ğŸ¯ **Smart ordering** (Q-learning) + **Smart execution** (Planner)
- ğŸ¯ **Learn what works** (Q-learning) + **Validate before running** (Planner)
- ğŸ¯ **Strategic** (which agent) + **Tactical** (should it run now)

---

## ğŸ”„ Potential Conflict Resolution

### **Scenario**: Q-Learning vs Planner Disagreement

```
Q-Learning says: "Run Processor next" (high Q-value)
Planner says: "Block Processor" (no Fetcher data available yet)

Resolution:
1. Planner wins (safety first!)
2. Agent is blocked
3. Q-learning observes negative reward
4. Q-value for "Run Processor without Fetcher data" decreases
5. Next time: Q-learning learns NOT to select Processor in that state
```

**This is LEARNING IN ACTION!** Q-learning discovers through Planner feedback what works and what doesn't.

---

## ğŸ“Š Configuration Options

### **Disable Planner (Trust Q-Learning Completely)**
```python
config = JottyConfig(
    enable_rl=True,
    enable_architect=False  # No Planner validation
)
# Result: Q-learning has full control, no safety checks
```

### **Enable Both (Recommended)**
```python
config = JottyConfig(
    enable_rl=True,
    enable_architect=True  # Planner validates
)
# Result: Q-learning learns optimal order, Planner ensures safety
```

### **Planner Only (No RL)**
```python
config = JottyConfig(
    enable_rl=False,
    enable_architect=True  # Planner validates fixed order
)
# Result: Fixed sequential order, Planner blocks unsafe executions
```

---

## ğŸ’¡ Real-World Analogy

### **Q-Learning = Strategic Planning**
*"Based on past experience, we should do Fetcher first, then Processor, then Visualizer"*

### **Planner = Tactical Validation**
*"Wait, we don't have the database credentials yet. Let's not run Fetcher right now."*

### **Together**:
- âœ… Q-Learning learns the ideal sequence over many episodes
- âœ… Planner ensures each step is safe given current context
- âœ… RL learns from Planner's blocks (low reward) and approvals (high reward)

---

## ğŸ¯ Summary

| Question | Answer |
|----------|--------|
| **Do they conflict?** | No - they operate at different levels (strategic vs tactical) |
| **Which runs first?** | Q-learning selects agent, then Planner validates |
| **Can Planner override Q-learning?** | Yes - Planner can block execution for safety |
| **Does RL learn from Planner blocks?** | Yes! Blocked â†’ negative reward â†’ Q-value decreases |
| **Should I use both?** | Yes (recommended) - Q-learning for ordering, Planner for safety |

---

## ğŸ“ Code Evidence

### **Q-Learning Selection** (`roadmap.py:584-678`)
```python
def get_next_task(self, q_predictor=None, current_state=None, goal=None, epsilon=0.1):
    """Select next task based on Q-values (Îµ-greedy)"""

    # Get Q-value for each available task
    for task in available_tasks:
        q_value, _, _ = q_predictor.predict_q_value(current_state, action, goal)

    # Select best Q-value (exploitation) or random (exploration)
    return best_task
```

### **Planner Validation** (`inspector.py`)
```python
class InspectorAgent:
    """Planner (Architect) and Reviewer (Auditor) validation"""

    def validate(self, actor, inputs):
        """Validate if actor should proceed"""

        # Planner checks preconditions
        result = self.agent(inputs)

        return ValidationResult(
            should_proceed=result.should_proceed,  # True/False
            reasoning=result.reasoning
        )
```

### **Integration** (`conductor.py`)
```python
async def run(self, goal):
    """Main execution loop"""

    # 1. Q-LEARNING: Select next task
    task = self.todo.get_next_task(
        q_predictor=self.q_learner,  # RL-based selection
        current_state=state,
        goal=goal
    )

    # 2. PLANNER: Validate execution
    if self.config.enable_architect:
        plan_result = await self._run_architect_for_actor(task.actor)
        if not plan_result.should_proceed:
            # Blocked! RL will learn from this
            return EpisodeResult(success=False, ...)

    # 3. EXECUTE: Run agent
    result = await self._execute_actor(task.actor)

    # 4. RL LEARNS: Update Q-values based on outcome
    reward = self._compute_reward(result)
    self.q_learner.update(state, action, reward)
```

---

**Generated**: 2026-01-17
**Purpose**: Clarify Q-Learning + Planner interaction
**Conclusion**: They work TOGETHER, not in conflict! ğŸ¤
