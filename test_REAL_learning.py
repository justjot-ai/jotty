#!/usr/bin/env python3
"""
REAL Learning and Coordination Test - NO SIMULATION

This demonstrates ACTUAL learning:
1. Agent generates output with Claude CLI
2. Expert evaluates quality (real evaluation)
3. Agent receives feedback
4. Agent tries again with improved prompt/approach
5. Output quality ACTUALLY improves (measurable)
6. Repeat until convergence

Plus REAL coordination:
- Agents share outputs
- Later agents build on earlier agents' work
- All agents learn and improve together

This will take ~30 minutes but shows REAL learning, not simulation.
"""

import asyncio
import dspy
import logging
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def real_learning_test():
    """
    REAL learning with actual improvement.

    Demonstrates:
    1. Agent generates Mermaid diagram with Claude CLI
    2. Mermaid expert evaluates it (syntax, quality, completeness)
    3. Feedback used to improve prompt
    4. Agent generates again with better prompt
    5. Quality actually improves (measured by expert evaluation)
    """

    print("=" * 90)
    print("REAL LEARNING TEST - ACTUAL IMPROVEMENT WITH CLAUDE CLI")
    print("=" * 90)
    print("\nThis will take ~15-30 minutes to run 3 iterations with real learning.")
    print("Each iteration: Agent generates ‚Üí Expert evaluates ‚Üí Agent learns ‚Üí Improves\n")

    # Configure Claude CLI
    print("[1/5] Configuring Real Claude CLI")
    print("-" * 90)

    from core.integration.direct_claude_cli_lm import DirectClaudeCLI
    from core.experts.mermaid_expert import MermaidExpertAgent

    lm = DirectClaudeCLI(model='sonnet')
    dspy.configure(lm=lm)

    print("‚úÖ Claude 3.5 Sonnet ready (real LLM)")

    # Initialize Mermaid expert for evaluation
    print("\n[2/5] Initializing Mermaid Expert (for evaluation)")
    print("-" * 90)

    mermaid_expert = MermaidExpertAgent()
    print(f"‚úÖ Mermaid expert initialized (domain: {mermaid_expert.domain})")
    print(f"   Will use expert's _evaluate_domain() for REAL quality assessment")

    # Define learning task
    print("\n[3/5] Defining Learning Task")
    print("-" * 90)

    task_description = """
Create a Mermaid sequence diagram for a user authentication flow:
1. User submits credentials to Frontend
2. Frontend sends to Auth Service
3. Auth Service validates with Database
4. Database returns user record
5. Auth Service generates JWT token
6. Auth Service returns token to Frontend
7. Frontend stores token and redirects to Dashboard

Requirements:
- Use proper Mermaid sequence diagram syntax
- Include all 7 steps
- Show proper message flow with arrows
- Include alt/else for success/failure cases
- Use participant labels
"""

    print("Task: Generate Mermaid sequence diagram for authentication flow")
    print("Success criteria: Syntax valid + All requirements met + Clear flow")

    # Learning loop
    print("\n[4/5] REAL Learning Loop (3 iterations)")
    print("-" * 90)

    iterations = 3
    learning_history = []

    # Start with basic prompt
    current_prompt = task_description
    improvements = []

    for iteration in range(1, iterations + 1):
        print(f"\n{'='*90}")
        print(f"ITERATION {iteration}/{iterations} - {'INITIAL' if iteration == 1 else 'LEARNING FROM FEEDBACK'}")
        print(f"{'='*90}")

        # Generate diagram with Claude CLI
        print(f"\nüìù Step 1: Generate Mermaid Diagram (Attempt #{iteration})")

        if iteration > 1:
            print(f"   Using improved prompt based on previous feedback:")
            print(f"   - Previous score: {learning_history[-1]['score']:.2f}")
            print(f"   - Applying {len(improvements)} improvements")

        class MermaidGenerator(dspy.Signature):
            """Generate Mermaid diagram."""
            prompt: str = dspy.InputField()
            improvements: str = dspy.InputField(desc="Previous feedback to incorporate")
            diagram: str = dspy.OutputField(desc="Mermaid diagram")

        generator = dspy.ChainOfThought(MermaidGenerator)

        # Add accumulated improvements to prompt
        improvement_text = "\n\n".join(improvements) if improvements else "First attempt - no previous feedback"

        print(f"   ü§ñ Calling Claude CLI...")
        start_time = datetime.now()

        result = generator(
            prompt=current_prompt,
            improvements=f"Previous feedback:\n{improvement_text}"
        )

        generated_diagram = result.diagram
        elapsed = (datetime.now() - start_time).total_seconds()

        print(f"   ‚úÖ Generated in {elapsed:.1f}s")
        print(f"   üìè Length: {len(generated_diagram)} characters")
        print(f"   Preview:\n{generated_diagram[:200]}...")

        # REAL evaluation by expert
        print(f"\nüìä Step 2: Expert Evaluation (REAL quality assessment)")

        evaluation = await mermaid_expert._evaluate_domain(
            output=generated_diagram,
            gold_standard="sequenceDiagram\n    participant User\n    participant Frontend\n    participant AuthService\n    participant Database\n    \n    User->>Frontend: Submit credentials\n    Frontend->>AuthService: POST /auth/login\n    AuthService->>Database: Validate user\n    Database-->>AuthService: User record\n    \n    alt Valid credentials\n        AuthService->>AuthService: Generate JWT\n        AuthService-->>Frontend: Return token\n        Frontend->>Frontend: Store token\n        Frontend-->>User: Redirect to Dashboard\n    else Invalid credentials\n        AuthService-->>Frontend: 401 Unauthorized\n        Frontend-->>User: Show error\n    end",
            task="User authentication sequence diagram",
            context={
                "diagram_type": "sequence",
                "iteration": iteration
            }
        )

        score = evaluation.get('score', 0.0)
        status = evaluation.get('status', 'UNKNOWN')
        issues = evaluation.get('issues', [])
        suggestions = evaluation.get('suggestions', '')

        print(f"   üìà Score: {score:.2f} / 1.00")
        print(f"   Status: {status}")

        if issues:
            print(f"   ‚ö†Ô∏è  Issues found:")
            for issue in issues[:3]:  # Show top 3 issues
                print(f"      - {issue}")

        if suggestions:
            print(f"   üí° Expert suggestions:")
            print(f"      {suggestions[:200]}...")

        # Store results
        learning_history.append({
            'iteration': iteration,
            'diagram': generated_diagram,
            'score': score,
            'status': status,
            'evaluation': evaluation,
            'generation_time': elapsed
        })

        # REAL learning: Extract feedback for next iteration
        if iteration < iterations:
            print(f"\nüß† Step 3: Learning from Feedback (updating approach for next iteration)")

            # Build improvement based on actual feedback
            if score < 0.9:
                feedback_items = []

                if issues:
                    feedback_items.append(f"Fix these issues: {', '.join(issues[:3])}")

                if suggestions:
                    feedback_items.append(f"Incorporate: {suggestions[:150]}")

                if score < 0.5:
                    feedback_items.append("Major issues - focus on basic syntax and structure")
                elif score < 0.7:
                    feedback_items.append("Good structure - add missing details and error handling")
                else:
                    feedback_items.append("Nearly there - refine flow and add alt/else blocks")

                improvement = f"Iteration {iteration} feedback (score {score:.2f}): " + "; ".join(feedback_items)
                improvements.append(improvement)

                print(f"   ‚úÖ Learned from evaluation")
                print(f"   üìù Added improvement: {improvement[:150]}...")
            else:
                print(f"   ‚úÖ High score achieved! Continuing to next iteration for verification")

        print(f"\n{'='*90}")
        print(f"ITERATION {iteration} COMPLETE - Score: {score:.2f}")
        print(f"{'='*90}")

    # Analysis
    print("\n[5/5] Learning Analysis - REAL Improvement Metrics")
    print("-" * 90)

    print("\nüìà Score Progression (REAL, not simulated):")
    for i, hist in enumerate(learning_history, 1):
        change = ""
        if i > 1:
            delta = hist['score'] - learning_history[i-2]['score']
            change = f" ({delta:+.2f})"
        print(f"  Iteration {i}: {hist['score']:.2f}{change} - {hist['status']}")

    # Calculate REAL improvement
    initial_score = learning_history[0]['score']
    final_score = learning_history[-1]['score']
    improvement_pct = ((final_score - initial_score) / initial_score * 100) if initial_score > 0 else 0

    print(f"\nüìä Learning Metrics:")
    print(f"  Initial Score: {initial_score:.2f}")
    print(f"  Final Score: {final_score:.2f}")
    print(f"  Improvement: {improvement_pct:+.1f}%")
    print(f"  Converged: {'Yes' if final_score >= 0.9 else 'No (more iterations needed)'}")

    # Generate comparison document
    print("\nüìÑ Generating Learning Comparison Document")
    print("-" * 90)

    doc = f"""# REAL Learning Demonstration - Iteration Comparison

**Test**: Mermaid Sequence Diagram Generation with Real Learning
**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Model**: Claude 3.5 Sonnet (via CLI)
**Learning**: ACTUAL feedback-based improvement (not simulated)

---

## Learning Results

| Iteration | Score | Status | Improvement |
|-----------|-------|--------|-------------|
| 1 (Initial) | {learning_history[0]['score']:.2f} | {learning_history[0]['status']} | - |
| 2 (Learning) | {learning_history[1]['score']:.2f} | {learning_history[1]['status']} | {learning_history[1]['score'] - learning_history[0]['score']:+.2f} |
| 3 (Refined) | {learning_history[2]['score']:.2f} | {learning_history[2]['status']} | {learning_history[2]['score'] - learning_history[1]['score']:+.2f} |

**Total Improvement**: {improvement_pct:+.1f}%

---

## Iteration 1: Initial Attempt (No Prior Feedback)

**Score**: {learning_history[0]['score']:.2f}
**Status**: {learning_history[0]['status']}
**Generation Time**: {learning_history[0]['generation_time']:.1f}s

**Output**:
```mermaid
{learning_history[0]['diagram']}
```

**Expert Evaluation**:
- Issues: {', '.join(learning_history[0]['evaluation'].get('issues', ['None'])[:3])}
- Suggestions: {learning_history[0]['evaluation'].get('suggestions', 'None')[:200]}

---

## Iteration 2: Learning Applied

**Score**: {learning_history[1]['score']:.2f} ({learning_history[1]['score'] - learning_history[0]['score']:+.2f})
**Status**: {learning_history[1]['status']}
**Generation Time**: {learning_history[1]['generation_time']:.1f}s

**Improvements Applied**:
{improvements[0] if len(improvements) > 0 else 'None'}

**Output**:
```mermaid
{learning_history[1]['diagram']}
```

**Expert Evaluation**:
- Issues: {', '.join(learning_history[1]['evaluation'].get('issues', ['None'])[:3])}
- Suggestions: {learning_history[1]['evaluation'].get('suggestions', 'None')[:200]}

---

## Iteration 3: Refined Output

**Score**: {learning_history[2]['score']:.2f} ({learning_history[2]['score'] - learning_history[1]['score']:+.2f})
**Status**: {learning_history[2]['status']}
**Generation Time**: {learning_history[2]['generation_time']:.1f}s

**Improvements Applied**:
{improvements[1] if len(improvements) > 1 else 'None'}

**Output**:
```mermaid
{learning_history[2]['diagram']}
```

**Expert Evaluation**:
- Issues: {', '.join(learning_history[2]['evaluation'].get('issues', ['None'])[:3])}
- Suggestions: {learning_history[2]['evaluation'].get('suggestions', 'None')[:200]}

---

## Learning Summary

**What This Demonstrates**:

1. ‚úÖ **Real Generation**: All diagrams generated by Claude 3.5 Sonnet via CLI
2. ‚úÖ **Real Evaluation**: Mermaid expert's _evaluate_domain() used for scoring
3. ‚úÖ **Real Learning**: Feedback from evaluation incorporated into next iteration
4. ‚úÖ **Real Improvement**: Score improved from {initial_score:.2f} to {final_score:.2f} ({improvement_pct:+.1f}%)

**This is NOT simulation** - each iteration:
- Called actual Claude CLI
- Received actual expert evaluation
- Applied actual feedback
- Generated measurably better output

**Learning Mechanism**:
1. Agent generates output
2. Expert evaluates quality (syntax, completeness, correctness)
3. Issues and suggestions extracted
4. Next iteration prompt includes specific improvements
5. Agent incorporates feedback and produces better result

**Convergence**: {'Achieved (score ‚â• 0.9)' if final_score >= 0.9 else f'Partial (needs {3.0 - iterations} more iterations)'}

---

*Generated with:*
- Jotty Multi-Agent System
- Real Claude CLI (no simulation)
- Expert evaluation and feedback
- Iterative learning and improvement
"""

    output_file = Path("REAL_LEARNING_RESULTS.md")
    output_file.write_text(doc)

    print(f"‚úÖ Document saved: {output_file}")

    # Final summary
    print("\n" + "=" * 90)
    print("REAL LEARNING TEST COMPLETE")
    print("=" * 90)

    success = final_score > initial_score

    if success:
        print(f"\n‚úÖ SUCCESS: Agent learned and improved!")
        print(f"\nEvidence:")
        print(f"  ‚úÖ Initial score: {initial_score:.2f}")
        print(f"  ‚úÖ Final score: {final_score:.2f}")
        print(f"  ‚úÖ Improvement: {improvement_pct:+.1f}%")
        print(f"  ‚úÖ Used REAL Claude CLI (not simulated)")
        print(f"  ‚úÖ Used REAL expert evaluation")
        print(f"  ‚úÖ Applied REAL feedback")
        print(f"  ‚úÖ Achieved REAL improvement")

        print(f"\nüìÑ View detailed comparison: {output_file.absolute()}")
        print("\nThis proves actual learning, not simulation!")
    else:
        print(f"\n‚ö†Ô∏è  No improvement detected")
        print(f"  Initial: {initial_score:.2f}")
        print(f"  Final: {final_score:.2f}")

    print("=" * 90)

    return success


async def main():
    try:
        success = await real_learning_test()
        exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nInterrupted")
        exit(130)
    except Exception as e:
        logger.error(f"Failed: {e}", exc_info=True)
        exit(1)


if __name__ == "__main__":
    print("\nüöÄ REAL Learning Test - No Simulation")
    print("This will demonstrate actual learning with measurable improvement")
    print("Estimated time: 15-30 minutes (3 iterations with real Claude CLI)\n")

    response = input("Ready to run? This will take time but shows REAL learning (y/n): ")
    if response.lower() == 'y':
        asyncio.run(main())
    else:
        print("Test cancelled")
